EEG-GazeCL

Contrastive Spatial Decoding of Gaze from EEG

EEG-GazeCL is a deep learning framework for decoding spatial gaze targets from EEG signals.
It learns spatially consistent embeddings using a Vision-Transformer-style EEG encoder and distance-aware contrastive learning.

Evaluated on the EEGEyeNet dataset, EEG-GazeCL achieves improved spatial accuracy, reduced catastrophic errors, and smoother gaze predictions compared to standard classification baselines.

ğŸ” Key Ideas

EEG gaze decoding is inherently spatial, not just categorical.

Standard cross-entropy treats all errors equally â€” spatial contrastive learning differentiates nearby vs. far errors.

Nearby gaze targets â†’ nearby embeddings; far targets â†’ well-separated embeddings.

Regularization is applied early in training, then annealed.

ğŸ§  Model Overview

Input: EEG trials of shape (128 channels Ã— 500 time samples)

Backbone: EEG Vision Transformer (EEGViT)

Temporal convolution â†’ creates time patches

Spatial convolution â†’ groups channels

Transformer encoder with CLS token

Outputs:

Gaze class (discrete dot)

Latent embedding (used for spatial regularization)

ğŸ“ Spatial Contrastive Learning

We introduce a distance-aware spatial contrastive loss:

âˆ¥
ğ‘§
ğ‘–
âˆ’
ğ‘§
ğ‘—
âˆ¥
âˆ
âˆ¥
ğ‘”
ğ‘–
âˆ’
ğ‘”
ğ‘—
âˆ¥
âˆ¥z
i
	â€‹

âˆ’z
j
	â€‹

âˆ¥âˆâˆ¥g
i
	â€‹

âˆ’g
j
	â€‹

âˆ¥

Where:

ğ‘§
ğ‘–
z
i
	â€‹

 = EEG embedding

ğ‘”
ğ‘–
g
i
	â€‹

 = 2D gaze position

Key design choices:

Contrastive loss applied only in early epochs (annealing)

Gaussian distance weighting

Combined with standard cross-entropy

ğŸ“Š Evaluation Metrics

Beyond standard classification accuracy, we measure:

Median pixel error

Mean pixel error

P(exact): exact dot prediction

P(within 1 step): nearest neighbor

P(within 2 steps): two spatial steps

This gives a faithful evaluation of spatial gaze decoding quality.

ğŸ“ Dataset

EEGEyeNet â€“ Position Task (Dot Targets)

Each subject file contains:

EEG: (N, T, C) EEG trials

labels: (N, 3) â†’ [trial_id, x, y]

Multiple subjects are stored as separate .npz files and merged during training.

ğŸ§ª Results (Single Subject)

After spatial contrastive learning + annealing:

Exact accuracy: ~45%

Within 1 step: ~54%

Within 2 steps: ~71%

Median pixel error: ~55 px

Errors are mostly local, with rare catastrophic failures.

ğŸš€ Project Structure
EEG-GazeCL/
â”œâ”€â”€ data/                # EEGEyeNet subject files
â”œâ”€â”€ models/              # EEGViT and loss implementations
â”œâ”€â”€ notebooks/           # Colab / Jupyter demos
â”œâ”€â”€ training/            # Training scripts
â”œâ”€â”€ evaluation/          # Metrics & plotting scripts
â”œâ”€â”€ utils/               # Data loaders, preprocessing
â””â”€â”€ README.md
